{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu28IxReZxcR",
        "outputId": "4e5f1548-34c1-40f9-f798-e7e701ae3449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Si32U9x-8wzq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf.keras.preprocessing.sequence import pad_sequences\n",
        "from tf.keras.utils import to_categorical\n",
        "from tf.keras.utils import plot_model\n",
        "from tf.keras.models import Model\n",
        "from tf.keras.layers import Input\n",
        "from tf.keras.layers import Dense, BatchNormalization\n",
        "from tf.keras.layers import LSTM\n",
        "from tf.keras.layers import Embedding\n",
        "from tf.keras.layers import Dropout\n",
        "from tf.keras.callbacks import ModelCheckpoint\n",
        "from tf.keras.preprocessing.image import load_img, img_to_array\n",
        "from tf.keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.merge import add\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from numpy import array\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, time, os, warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnpEMWQ6dlA6",
        "outputId": "a95c972a-cddf-42c5-d5c1-17f84a605624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Images in Dataset = 24222\n"
          ]
        }
      ],
      "source": [
        "image_path = \"/content/gdrive/MyDrive/Flick8k/Flick8k_Dataset\"\n",
        "dir_Flickr_text = '/content/gdrive/MyDrive/Flick8k/Flick8k_Text/Flickr8k.token.txt'\n",
        "\n",
        "jpgs = os.listdir(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "sd-xPRq-eMw8",
        "outputId": "a2523ab1-32d7-4235-890a-95cedb331907"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>filename</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a girl going into a wooden building .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a little girl climbing into a wooden playhouse .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  index  ...                                            caption\n",
              "0     0  ...  a child in a pink dress is climbing up a set o...\n",
              "1     1  ...              a girl going into a wooden building .\n",
              "2     2  ...   a little girl climbing into a wooden playhouse .\n",
              "3     3  ...  a little girl climbing the stairs to her playh...\n",
              "4     4  ...  a little girl in a pink dress going into a woo...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print(os.path.exists('/content/gdrive/MyDrive/Flick8k/Flick8k_Text/Flickr8k.token.txt'))\n",
        "file = open(dir_Flickr_text,'r')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "datatxt = []\n",
        "for line in text.split('\\n'):\n",
        "    col = line.split('\\t')\n",
        "    if len(col) == 1:\n",
        "        continue\n",
        "    w = col[0].split(\"#\")\n",
        "    datatxt.append(w + [col[1].lower()])\n",
        "\n",
        "data = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n",
        "data = data.reindex(columns =['index','filename','caption'])\n",
        "data = data[data['filename'] != '2258277193_586949ec62.jpg.1']\n",
        "\n",
        "\n",
        "uni_filenames = np.unique(data.filename.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZfHfLEIgRra",
        "outputId": "6196c9ac-7e86-4b9c-bd16-d30f29426fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 8918\n"
          ]
        }
      ],
      "source": [
        "vocabulary = []\n",
        "for txt in data.caption.values:\n",
        "    vocabulary.extend(txt.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yIaD4ggfg0mc"
      },
      "outputs": [],
      "source": [
        "img = data[\"filename\"].tolist()\n",
        "caption = data[\"caption\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_1uklMU2P27-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(string.punctuation)\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "def remove_numeric(text):\n",
        "    text_no_numeric = \"\"\n",
        "    for word in text.split():\n",
        "        isalpha = word.isalpha()\n",
        "        if isalpha:\n",
        "            text_no_numeric += \" \" + word\n",
        "    return(text_no_numeric)\n",
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    text = remove_numeric(text)\n",
        "    return(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MdEmhA-wPzpW"
      },
      "outputs": [],
      "source": [
        "for i, caption in enumerate(data.caption.values):\n",
        "    newcaption = text_clean(caption)\n",
        "    data[\"caption\"].iloc[i] = newcaption\n",
        "\n",
        "clean_vocabulary = []\n",
        "for txt in data.caption.values:\n",
        "    clean_vocabulary.extend(txt.split())\n",
        "\n",
        "all_captions = []\n",
        "for caption  in data[\"caption\"].astype(str):\n",
        "    caption = '<start> ' + caption+ ' <end>'\n",
        "    all_captions.append(caption)\n",
        "all_captions[:10]\n",
        "\n",
        "all_img_name_vector = []\n",
        "for annot in data[\"filename\"]:\n",
        "    full_image_path = image_path +'/'+ annot\n",
        "\n",
        "    all_img_name_vector.append(full_image_path)\n",
        "all_img_name_vector[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bxxePX6TGTuF"
      },
      "outputs": [],
      "source": [
        "def data_limiter(num,total_captions,all_img_name_vector):\n",
        "  train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n",
        "  train_captions = train_captions[:num]\n",
        "  img_name_vector = img_name_vector[:num]\n",
        "  return train_captions,img_name_vector\n",
        "  \n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzJI3CC3cdRP",
        "outputId": "03238907-c05b-4159-a3de-d34bf4ba46eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "train_captions,img_name_vector = data_limiter(40000,all_captions,all_img_name_vector)\n",
        "\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tGPHHZlchUq",
        "outputId": "43ff2b20-8245-4037-897f-9a903452d737"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 4/506 [00:30<1:02:57,  7.52s/it]"
          ]
        }
      ],
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(20)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "  # print('we got here')\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(str(path_of_feature).split('.jpg')[0], bf.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMLfDYAfdH93"
      },
      "source": [
        "### Preprocess and tokenize the captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWXKqpL5dBss"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000,\n",
        "                                                  oov_token=\"<bad>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVq2KakYdVN9"
      },
      "source": [
        "### Split the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OKymw5Hd037"
      },
      "outputs": [],
      "source": [
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU_j7m48eGSn"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "num_steps = len(img_name_train) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anmVgUjzeZOw"
      },
      "outputs": [],
      "source": [
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlChoPCGeab0"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRFegUqmINJr"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNARmUUYMbyi"
      },
      "outputs": [],
      "source": [
        "def positional_encoding_1d(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAjxo7YlMg6p"
      },
      "outputs": [],
      "source": [
        "def positional_encoding_2d(row,col,d_model):\n",
        "  assert d_model % 2 == 0\n",
        "  # first d_model/2 encode row embedding and second d_model/2 encode column embedding\n",
        "  row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n",
        "  col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n",
        "  angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
        "  angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
        "  #apply sin and cos to odd and even indices resp.\n",
        "  angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
        "  angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
        "  angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
        "  angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
        "  pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPtqOKOfiNM6"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od9Ltf9qkJiu"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jPObHU0k2SC"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "  \n",
        "  # adjust matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add mask to tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iduottdan-sp"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask=None):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF7Zq49KqvDw"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks9Oo02jsN3D"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask=None):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)\n",
        "    \n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3ZXI1RVw2Rf"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask=None, padding_mask=None):\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6wYw3lpyYWL"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "               row_size,col_size,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n",
        "    self.pos_encoding = positional_encoding_2d(row_size,col_size, \n",
        "                                            self.d_model)\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask=None):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    x = self.dropout(x, training=training)\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOQ_DNET2kgO"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask=None, padding_mask=None):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv5-Vrse4_uK"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff,row_size,col_size, \n",
        "               target_vocab_size,max_pos_encoding, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,row_size,col_size, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size,max_pos_encoding, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training,look_ahead_mask=None, dec_padding_mask=None,enc_padding_mask=None):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz2eNi5G7RkQ"
      },
      "outputs": [],
      "source": [
        "num_layer = 4\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "row_size = 8\n",
        "col_size = 8\n",
        "target_vocab_size = 5001\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsjF1eoTT3eQ"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SSH-4bOTJff"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JHB2oUSUmX5"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkkzdkRvUh4A"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqXuReTiU-fq"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3LHMBwxVohZ"
      },
      "outputs": [],
      "source": [
        "def create_masks_decoder(tar):\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  return combined_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW12LgraY3sP"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  dec_mask = create_masks_decoder(tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(img_tensor, tar_inp, \n",
        "                                 True,  \n",
        "                                 dec_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM9u-U3BVEW0"
      },
      "outputs": [],
      "source": [
        "if os.path.exists('/content/gdrive/MyDrive/Flick8k/image_caption_transformer_45.h5'):\n",
        "  new_model = tf.keras.models.load_model('/content/gdrive/MyDrive/Flick8k/image_caption_transformer_45.h5')\n",
        "  new_model.summary()\n",
        "else:\n",
        "  transformer = Transformer(num_layer,d_model,num_heads,dff,row_size,col_size,target_vocab_size,max_pos_encoding=target_vocab_size,rate=dropout_rate)\n",
        "\n",
        "  for epoch in range(30):\n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (img_tensor, tar)) in enumerate(dataset):\n",
        "      train_step(img_tensor, tar)\n",
        "      if batch % 50 == 0:\n",
        "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                  train_loss.result(), \n",
        "                                                  train_accuracy.result()))\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "    transformer.save_weights('/content/gdrive/MyDrive/Flick8k/image_caption_transformer_45.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5r6nWrh1D5r"
      },
      "outputs": [],
      "source": [
        "def evaluate(image):\n",
        "\n",
        "  temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "  img_tensor_val = image_features_extract_model(temp_input)\n",
        "  img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "  start_token = tokenizer.word_index['<start>']\n",
        "  end_token = tokenizer.word_index['<end>']\n",
        "   \n",
        "  decoder_input = [start_token]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "  result = []\n",
        "\n",
        "  for i in range(100):\n",
        "      dec_mask = create_masks_decoder(output)\n",
        "      predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n",
        "      predictions = predictions[: ,-1:, :]\n",
        "      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "      if predicted_id == end_token:\n",
        "          return result,tf.squeeze(output, axis=0), attention_weights\n",
        "          \n",
        "      result.append(tokenizer.index_word[int(predicted_id)])\n",
        "      output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return result,tf.squeeze(output, axis=0), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkQuofIR3XXy"
      },
      "outputs": [],
      "source": [
        "start_token = tokenizer.word_index['<start>']\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "caption,result,attention_weights = evaluate(image)\n",
        "\n",
        "first = real_caption.split(' ', 1)[1]\n",
        "real_caption = first.rsplit(' ', 1)[0]\n",
        "for i in caption:\n",
        "    if i==\"<bad>\":\n",
        "        caption.remove(i)\n",
        "\n",
        "for i in real_caption:\n",
        "    if i==\"<bad>\":\n",
        "        real_caption.remove(i)       \n",
        "result_join = ' '.join(caption)\n",
        "result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = caption\n",
        "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "print(f\"BLEU-1 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "print(f\"BLEU-2 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "print(f\"BLEU-3 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "print(f\"BLEU-4 score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Predicted Caption:', ' '.join(caption))\n",
        "temp_image = np.array(Image.open(image))\n",
        "plt.imshow(temp_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvUa6VCHSEzd"
      },
      "outputs": [],
      "source": [
        "start_token = tokenizer.word_index['<start>']\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "caption,result,attention_weights = evaluate(image)\n",
        "\n",
        "first = real_caption.split(' ', 1)[1]\n",
        "real_caption = first.rsplit(' ', 1)[0]\n",
        "\n",
        "for i in caption:\n",
        "    if i==\"<bad>\":\n",
        "        caption.remove(i)\n",
        "\n",
        "for i in real_caption:\n",
        "    if i==\"<bad>\":\n",
        "        real_caption.remove(i)\n",
        "              \n",
        "result_join = ' '.join(caption)\n",
        "result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = caption\n",
        "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "print(f\"BLEU-1 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "print(f\"BLEU-2 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "print(f\"BLEU-3 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "print(f\"BLEU-4 score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Predicted Caption:', ' '.join(caption))\n",
        "temp_image = np.array(Image.open(image))\n",
        "plt.imshow(temp_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG64jntxSUIf"
      },
      "outputs": [],
      "source": [
        "start_token = tokenizer.word_index['<start>']\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "no_images_to_display = 2\n",
        "for i in no_images_to_display\n",
        "    # select random image from validation data\n",
        "    rid = np.random.randint(0, len(img_name_val))\n",
        "    image = img_name_val[rid]\n",
        "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "    caption,result,attention_weights = evaluate(image)\n",
        "\n",
        "    first = real_caption.split(' ', 1)[1]\n",
        "    real_caption = first.rsplit(' ', 1)[0]\n",
        "\n",
        "    #remove \"<bad>\" in result\n",
        "    for i in caption:\n",
        "        if i==\"<bad>\":\n",
        "            caption.remove(i)\n",
        "\n",
        "    for i in real_caption:\n",
        "        if i==\"<bad>\":\n",
        "            real_caption.remove(i)\n",
        "\n",
        "    #remove <end> from result         \n",
        "    result_join = ' '.join(caption)\n",
        "    result_final = result_join.rsplit(' ', 1)[0]\n",
        "    real_appn = []\n",
        "    real_appn.append(real_caption.split())\n",
        "\n",
        "    print ('Real Captions:', real_appn)\n",
        "    print ('Predicted Caption:', ' '.join(caption))\n",
        "    temp_image = np.array(Image.open(image))\n",
        "    plt.imshow(temp_image)\n",
        "\n",
        "    reference = real_appn\n",
        "    candidate = caption\n",
        "    score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "    print(f\"BLEU-1 score: {score*100}\")\n",
        "    score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "    print(f\"BLEU-2 score: {score*100}\")\n",
        "    score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "    print(f\"BLEU-3 score: {score*100}\")\n",
        "    score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "    print(f\"BLEU-4 score: {score*100}\")\n",
        "    print(f\"METEOR score: {meteor_score(reference, candidate)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Caption_Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
