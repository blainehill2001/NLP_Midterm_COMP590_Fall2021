{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnxWdSz5_So2",
        "outputId": "d2052016-5332-4bdc-f3a8-29821f63c421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YezF3-VxTqSF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf.keras import Input, layers\n",
        "from tf.keras import optimizers\n",
        "from tf.keras.optimizers import Adam\n",
        "from tf.keras.preprocessing import sequence\n",
        "from tf.keras.preprocessing import image\n",
        "from tf.keras.preprocessing.text import Tokenizer\n",
        "from tf.keras.preprocessing.sequence import pad_sequences\n",
        "from tf.keras.applications.inception_v3 import InceptionV3\n",
        "from tf.keras.applications.inception_v3 import preprocess_input\n",
        "from tf.keras.utils import to_categorical\n",
        "from tf.keras.models import Model, load_model\n",
        "from tf.keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
        "\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from time import time\n",
        "\n",
        "token_path = \"/content/gdrive/MyDrive/Flick8k/Flick8k_Text/Flickr8k.token.txt\"\n",
        "train_images_path = '/content/gdrive/MyDrive/Flick8k/Flick8k_Text/Flickr_8k.trainImages.txt'\n",
        "test_images_path = '/content/gdrive/MyDrive/Flick8k/Flick8k_Text/Flickr_8k.testImages.txt'\n",
        "images_path = \"/content/gdrive/MyDrive/Flick8k/Flick8k_Dataset/\"\n",
        "glove_path = '/content/gdrive/MyDrive/Flick8k/Flick8k_Text/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RH3CAjtuwJYR"
      },
      "outputs": [],
      "source": [
        "#create dict with image name as key and captions as values\n",
        "flickr8k = dict()\n",
        "for line in open(token_path,'r').read().split('\\n'):\n",
        "        pieces = line.split()\n",
        "        if len(line) > 2:\n",
        "          image_name = pieces[0].split('.')[0]\n",
        "          image_caption = ' '.join(pieces[1:])\n",
        "          if image_name not in flickr8k:\n",
        "              flickr8k[image_name] = list()\n",
        "          flickr8k[image_name].append(image_caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZeBNmfToTqSi"
      },
      "outputs": [],
      "source": [
        "#Convert to lowercase and remove punctuation \n",
        "for key, caption_list in flickr8k.items():\n",
        "    for i in range(len(caption_list)):\n",
        "        caption = caption_list[i].split()\n",
        "        caption = [word.lower() for word in caption]\n",
        "        caption = [word.translate(str.maketrans('', '', string.punctuation)) for word in caption]\n",
        "        caption_list[i] =  ' '.join(caption)\n",
        "#save cleaned captions\n",
        "lines = list()\n",
        "for key, caption_list in flickr8k.items():\n",
        "    for caption in caption_list:\n",
        "        lines.append(key + ' ' + caption)\n",
        "new_flickr8k = '\\n'.join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKHS0auD078Q",
        "outputId": "5892be55-d7ec-4904-f27b-73c451a58af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset size: 6000\n"
          ]
        }
      ],
      "source": [
        "#load 6000 training image ids \n",
        "unique_training_images = set()\n",
        "for line in open(train_images_path,'r').read().split('\\n'):\n",
        "    if len(line) > 1:\n",
        "      name = line.split('.')[0]\n",
        "      unique_training_images.add(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szONWSjRTqSv",
        "outputId": "44556f41-0572-43fc-9ef5-3032294b6fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Flick8k/Flick8k_Dataset/\n"
          ]
        }
      ],
      "source": [
        "#process image flickr8k\n",
        "img = glob.glob(images_path + '*.jpg')\n",
        "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
        "train_img = []\n",
        "for i in img: \n",
        "    if i[len(images_path):] in train_images:\n",
        "        train_img.append(i)\n",
        "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
        "test_img = []\n",
        "for i in img: \n",
        "    if i[len(images_path):] in test_images: \n",
        "        test_img.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tdR6udI3Jda",
        "outputId": "8fdf6605-8daa-46ec-a2e8-980d9649ba62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Descriptions: train = 6000\n"
          ]
        }
      ],
      "source": [
        "#load flickr8k into dict and append start and end token\n",
        "train_flickr8k = dict()\n",
        "for line in new_flickr8k.split('\\n'):\n",
        "    pieces = line.split()\n",
        "    image_name, image_caption = pieces[0], pieces[1:]\n",
        "    if image_name in unique_training_images:\n",
        "        if image_name not in train_flickr8k:\n",
        "            train_flickr8k[image_name] = list()\n",
        "        caption = ' '.join(image_caption)\n",
        "        caption = '<start> ' + caption + ' <end>'\n",
        "        train_flickr8k[image_name].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHAmGA2vTqTG",
        "outputId": "3a3e58c1-2bbb-4efd-df04-15770404f5c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#create big caption list\n",
        "big_caption_list = []\n",
        "for key, value in train_flickr8k.items():\n",
        "    for caption in value:\n",
        "        big_caption_list.append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BNfUWfUTqTI",
        "outputId": "adb3d807-db51-48eb-98c3-86209a052e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary = 1659\n"
          ]
        }
      ],
      "source": [
        "#reduce vocabulary \n",
        "max_count = 15\n",
        "counts = {}\n",
        "i = 0\n",
        "for caption in big_caption_list:\n",
        "    i += 1\n",
        "    for word in caption.split(' '):\n",
        "        counts[word] = counts.get(word, 0) + 1\n",
        "vocab = [word for word in counts if counts[word] >= max_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oFm0u7kyTqTJ"
      },
      "outputs": [],
      "source": [
        "index_to_word = {}\n",
        "word_to_index = {}\n",
        "\n",
        "index = 1\n",
        "for word in vocab:\n",
        "    word_to_index[word] = index\n",
        "    index_to_word[index] = word\n",
        "    index += 1\n",
        "\n",
        "#set max_length_of_caption\n",
        "max_length_of_caption = 40\n",
        "vocab_size = 1 + len(index_to_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fwvl_1L6TqTR"
      },
      "outputs": [],
      "source": [
        "#using glove.6b.200.txt\n",
        "embeddings_index = {} \n",
        "for line in open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\"):\n",
        "    embeddings_index[line.split()[0]] = np.asarray(line.split()[1:], dtype='float32')\n",
        "\n",
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in word_to_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "s2OZ22cmTqS5"
      },
      "outputs": [],
      "source": [
        "#using InceptionV3 for transfer learning\n",
        "model = InceptionV3(weights='imagenet')\n",
        "#removing classification layers\n",
        "model_new = Model(model.input, model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4OqH1PRgTqTO"
      },
      "outputs": [],
      "source": [
        "#train function\n",
        "def data_generator(flickr8k, images, word_to_index, max_length_of_caption, no_images):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, caption_list in flickr8k.items():\n",
        "            n+=1\n",
        "            # retrieve the image feature\n",
        "            image = images[key+'.jpg']\n",
        "            for caption in caption_list:\n",
        "                # encode the sequence\n",
        "                seq = [word_to_index[word] for word in caption.split(' ') if word in word_to_index]\n",
        "                for i in range(1, len(seq)):\n",
        "                    X1.append(image)\n",
        "                    X2.append(pad_sequences([seq[:i]], maxlen=max_length_of_caption)[0])\n",
        "                    y.append(to_categorical([seq[i]], num_classes=vocab_size)[0])\n",
        "                    \n",
        "            if n==no_images:\n",
        "                yield ([array(X1), array(X2)], array(y))\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QJQq5lXpmzk"
      },
      "outputs": [],
      "source": [
        "#reshape to 299x299 for InceptionV3 and preprocess\n",
        "def preprocess(image_path):\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def encode(image):\n",
        "    image = preprocess(image)\n",
        "    fea_vec = model_new.predict(image) \n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
        "    return fea_vec\n",
        "\n",
        "#run encoding on all training images\n",
        "encoding_train = {}\n",
        "for img in train_img: \n",
        "    encoding_train[img[len(images_path):]] = encode(img)\n",
        "#run encoding on all testing images\n",
        "encoding_test = {}\n",
        "for img in test_img:\n",
        "    encoding_test[img[len(images_path):]] = encode(img)\n",
        "\n",
        "#set features\n",
        "train_features = encoding_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gti-3SgUTqTU"
      },
      "outputs": [],
      "source": [
        "#training image\n",
        "inputs1 = Input(shape=(2048,))\n",
        "layer1 = Dense(256, activation='relu')(Dropout(0.5)(inputs1))\n",
        "#training caption\n",
        "inputs2 = Input(shape=(max_length_of_caption,))\n",
        "layer2 = LSTM(256)(Dropout(0.5)(Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)))\n",
        "#concatenate and decode\n",
        "decoder = Dense(256, activation='relu')(add([layer1, layer2]))\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=Dense(vocab_size, activation='softmax')(decoder))\n",
        "# model.summary()\n",
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ix_eqmTTqTc"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 5\n",
        "steps = len(train_flickr8k)//batch_size\n",
        "#Next let's train our model for 20 epochs with batch size of 5 and 2000 steps per epoch\n",
        "generator = data_generator(train_flickr8k, train_features, word_to_index, max_length_of_caption, batch_size)\n",
        "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)\n",
        "#save model\n",
        "model.save('/content/gdrive/MyDrive/Flick8k/lstm_cnn_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "n-upbWarTqTm"
      },
      "outputs": [],
      "source": [
        "def beam_search_predictions(image, beam_index = 4):\n",
        "    start = [word_to_index[\"<start>\"]]\n",
        "    initial_word = [[start, 0.0]]\n",
        "    while len(initial_word[0][0]) < max_length_of_caption:\n",
        "        temp = []\n",
        "        for i in initial_word:\n",
        "            par_caps = sequence.pad_sequences([i[0]], maxlen=max_length_of_caption, padding='post')\n",
        "            preds = model.predict([image,par_caps], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "\n",
        "            for word in word_preds:\n",
        "                next_caption, next_probability = i[0][:], i[1]\n",
        "                next_caption.append(word)\n",
        "                next_probability += preds[0][word]\n",
        "                temp.append([next_caption, next_probability])\n",
        "                    \n",
        "        initial_word = temp\n",
        "        \n",
        "        initial_word = sorted(initial_word, reverse=False, key=lambda x: x[1])\n",
        "        \n",
        "        initial_word = initial_word[-beam_index:]\n",
        "    \n",
        "    initial_word = initial_word[-1][0]\n",
        "    temp_caption = [index_to_word[i] for i in initial_word]\n",
        "    final_caption = []\n",
        "    for i in temp_caption:\n",
        "        if i == '<end>':\n",
        "            break\n",
        "        else:\n",
        "            final_caption.append(i)\n",
        "    \n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "Mc09XoAkTqTo",
        "outputId": "db540c58-d246-4d26-e286-0c8ead0176e9"
      },
      "outputs": [],
      "source": [
        "no_images_to_test = 2\n",
        "for i in range(no_images_to_test):\n",
        "    image_name = list(encoding_test.keys())[i]\n",
        "    image = encoding_test[image_name].reshape((1,2048))\n",
        "    x=plt.imread(images_path+image_name)\n",
        "    plt.imshow(x)\n",
        "    real_captions = flickr8k[str(image_name).split('.jpg')[0]]\n",
        "    predicted_caption = beam_search_predictions(image, beam_index = 4)\n",
        "    print(\"Real Captions:\", real_captions)\n",
        "    print(\"Beam Search, K = 4:\",beam_search_predictions(image, beam_index = 4))\n",
        "\n",
        "\n",
        "    scores = []\n",
        "    for caption in real_captions:\n",
        "        scores.append(sentence_bleu(caption, predicted_caption, weights=(1.0,0,0,0)))\n",
        "    print(f\"BLEU-1 score: {sum(scores)/len(scores)*100}\")\n",
        "    scores = []\n",
        "    for caption in real_captions:\n",
        "        scores.append(sentence_bleu(caption, predicted_caption, weights=(0.5,0.5,0,0)))\n",
        "    print(f\"BLEU-2 score: {sum(scores)/len(scores)*100}\")\n",
        "    scores = []\n",
        "    for caption in real_captions:\n",
        "        scores.append(sentence_bleu(caption, predicted_caption, weights=(0.33,0.33,0.33,0)))\n",
        "    print(f\"BLEU-3 score: {sum(scores)/len(scores)*100}\")\n",
        "    scores = []\n",
        "    for caption in real_captions:\n",
        "        scores.append(sentence_bleu(caption, predicted_caption, weights=(0.25,0.25,0.25,0.25)))\n",
        "    print(f\"BLEU-4 score: {sum(scores)/len(scores)*100}\")\n",
        "\n",
        "    print(f\"METEOR score: {meteor_score(real_captions, caption)*100}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "bEQ47FrSTqTw"
      ],
      "name": "merged_encoder_decoder_for_image_captioning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
